{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known Keyword Extracton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import Request, urlopen\n",
    "from tldextract import extract\n",
    "import progressbar\n",
    "import pandas as pd\n",
    "import certifi\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ua = UserAgent()\n",
    "header = {'User-Agent':str(ua.chrome)}\n",
    "\n",
    "links_substring_blacklist = [\n",
    "    'sitemap',\n",
    "    'terms',\n",
    "    'contact',\n",
    "    'careers',\n",
    "    'uploads',\n",
    "    'policy',\n",
    "    '/media',\n",
    "    'events/',\n",
    "    '.jpg',\n",
    "    '.png',\n",
    "    '.pdf'\n",
    "]\n",
    "keywords_limit = 15\n",
    "crawl_limit = 30\n",
    "\n",
    "def link_from_same_subdomain(url, domain):\n",
    "    _, sub_hostname, sub_suffix = extract(url)\n",
    "    _, hostname, suffix = extract(domain)\n",
    "    \n",
    "    return sub_hostname == hostname and sub_suffix == suffix\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    print(f\"getting text from url {url}\")\n",
    "    try:\n",
    "        html_page = requests.get(url, headers=header, verify=False, timeout=10).content\n",
    "    except Exception as e:\n",
    "        print(f'Failed to fetch {url}, {e}\\n')\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "    for elem in soup.find_all(['script', 'li', 'header', 'noscript', 'h1', 'h2', 'h3', 'h4']):\n",
    "        elem.decompose()\n",
    "\n",
    "    strips = list(soup.stripped_strings)\n",
    "    \n",
    "    return '\\n'.join([f' {strip} ' for strip in strips if len(strip) > 50])\n",
    "\n",
    "def get_links_from_url(url):\n",
    "    try:\n",
    "        print(f\"getting links url {url}\")\n",
    "        html_page = requests.get(url, headers=header, verify=False, timeout=10).content\n",
    "    except Exception as e:\n",
    "        print(f'Faile to fetch {url} {e}\\n')\n",
    "        return set()\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "    # get all links with absolute paths\n",
    "    links = set()\n",
    "    for link in soup.findAll('a'):\n",
    "        link_href = urljoin(url, link.get('href'))\n",
    "        parsed = urlparse(link_href, scheme='https')._replace(fragment=\"\")\n",
    "        \n",
    "        if parsed.scheme[:4] == 'http':\n",
    "            links.add(parsed.geturl().rstrip('/'))\n",
    "\n",
    "    # return urls with same base domain\n",
    "    return {link for link in links if link_from_same_subdomain(link, url)}\n",
    "\n",
    "def get_links_recursive(url, existing_links=set(), max_depth=1, maxlen=20):\n",
    "    if (len(existing_links) >= max_depth):\n",
    "        return set()\n",
    "\n",
    "    links = get_links_from_url(url)\n",
    "    \n",
    "    existing_links = existing_links | {url}\n",
    "    for link in links - existing_links:\n",
    "        links |= get_links_recursive(link, existing_links)\n",
    "        \n",
    "    for blacklisted_domain_substring in links_substring_blacklist:\n",
    "        links = [link for link in links if blacklisted_domain_substring not in link]\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_text_from_website(website):\n",
    "    text = ''\n",
    "    for link in get_links_recursive(website)[:crawl_limit]:\n",
    "        text += get_text_from_url(link) + '\\n'\n",
    "        \n",
    "    return text.replace('.', ' ').lower()\n",
    "\n",
    "def get_known_keywords_for_website(website):\n",
    "    print(f\"printing {website}\")\n",
    "    print(f\"printing {website}\")\n",
    "    try:\n",
    "        text = get_text_from_website(website)\n",
    "        known_keywords = pd.read_csv('known_keywords.csv').drop_duplicates()\n",
    "        known_keywords['keyword'] = known_keywords['keyword'].str.lower()\n",
    "        known_keywords['counts'] = known_keywords['keyword'].map(lambda x: text.count(f' {x.strip()} '))\n",
    "        known_keywords['keyword_word_count'] = known_keywords['keyword'].map(lambda x: len(x.split(' ')))\n",
    "\n",
    "        keyword_counts = known_keywords[known_keywords['counts'] > 0]\n",
    "        keyword_counts = keyword_counts.sort_values(by=['keyword_word_count', 'counts'], ascending=False)\n",
    "\n",
    "        return keyword_counts['keyword'].tolist()[:keywords_limit]\n",
    "    except Exception as e:\n",
    "        print(f'got an error with parsing {website}. Skipping. {e}')\n",
    "        return []\n",
    "\n",
    "def get_title(url):\n",
    "    _, hostname, suffix = extract(url)\n",
    "    return hostname.capitalize()\n",
    "\n",
    "safe_url_path = lambda url: \"keywords_parts/\" + re.sub(r'\\W+', '', url)\n",
    "\n",
    "def get_known_keywords_for_website_parallel(url):\n",
    "    keywords_part_path = safe_url_path(url)\n",
    "    if os.path.exists(keywords_part_path):\n",
    "        #print(f\"skipping, {keywords_part_path} already exists\")\n",
    "        return []\n",
    "    else:\n",
    "        keywords = get_known_keywords_for_website(url)\n",
    "        with open(keywords_part_path, 'wb') as fp:\n",
    "            pickle.dump(keywords, fp)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process domains from domains.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = pd.read_csv('domains.csv')\n",
    "domains[\"domain\"] = domains[\"url\"].map(lambda x: urlparse(x).scheme + '://' + urlparse(x).netloc)\n",
    "domains = domains[[\"domain\"]].drop_duplicates()\n",
    "\n",
    "urls = domains[\"domain\"]\n",
    "print(urls.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform hierarchical website scrapoing traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pool = multiprocessing.Pool()\n",
    "pool.map(get_known_keywords_for_website_parallel, urls)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dataframe with extracted keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_keywords = []\n",
    "for url in urls:\n",
    "    with open(safe_url_path(url), \"rb\") as f:\n",
    "        keywords = pickle.load(f)\n",
    "    extracted_keywords.append({\n",
    "        'domain': url,\n",
    "        'keywords': keywords,\n",
    "        'company_name': get_title(url)\n",
    "    })\n",
    "\n",
    "extracted_kw = pd.DataFrame(extracted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_kw = extracted_kw[extracted_kw[\"keywords\"].map(lambda x: len(x)) > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "blacklist = list()\n",
    "with open(\"domain_blacklist.txt\", \"r\") as f:\n",
    "    blacklist = f.read().splitlines()\n",
    "\n",
    "def filter_domains(domain):\n",
    "    domain = domain.replace('http://', '').replace('https://', '')\n",
    "    for blacklisted_domain in blacklist:\n",
    "        if fnmatch.fnmatch(domain, blacklisted_domain):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_kw_list = extracted_kw[extracted_kw['domain'].map(filter_domains)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_kw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_kw_list.to_csv(\"extracted_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown Keywords Extraction (POC use case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting links url https://www.lonza.com\n",
      "getting text from url https://www.lonza.com/news-and-media/image-library\n",
      "getting text from url https://www.lonza.com/public/conditions\n",
      "getting text from url https://www.lonza.com/company-overview/our-history\n",
      "getting text from url https://www.lonza.com/news-and-media/a-view-on-podcast\n",
      "getting text from url https://www.lonza.com/news-and-media/leadership-portraits\n",
      "getting text from url https://www.lonza.com/company-overview/leadership\n",
      "getting text from url https://pharma.lonza.com\n",
      "getting text from url https://www.lonza.com/news-and-media\n",
      "getting text from url https://www.lonza.com/investor-relations/shareholders-and-stock-information\n",
      "getting text from url https://www.lonza.com/sustainability/people\n",
      "getting text from url https://www.lonza.com/company-overview/our-locations\n",
      "getting text from url https://www.lonza.com/sustainability\n",
      "getting text from url https://www.lonza.com/news-and-media/news-archive\n",
      "getting text from url https://www.lonza.com/company-overview/strategy\n",
      "getting text from url https://www.lonza.com/news-and-media/logo-guidelines\n",
      "getting text from url https://www.lonza.com/investor-relations/corporate-governance\n",
      "getting text from url https://www.lonza.com/company-overview/our-websites\n",
      "getting text from url https://www.lonza.com/sustainability/performance\n",
      "getting text from url https://www.lonza.com/sustainability/global-quality\n",
      "getting text from url https://www.lonza.com/investor-relations/reporting-center\n",
      "getting text from url https://www.lonza.com/sustainability/ethics-and-compliance\n",
      "getting text from url https://www.lonza.com/company-overview\n",
      "getting text from url https://www.lonza.com/sustainability/planet\n",
      "getting text from url https://www.lonza.com/events\n",
      "getting text from url https://www.lonza.com/news-and-media/blog\n",
      "getting text from url https://www.lonza.com/sustainability/community\n",
      "getting text from url https://www.lonza.com/news-and-media/videos\n",
      "getting text from url https://pharma.lonza.com:443\n",
      "getting text from url https://www.lonza.com/investor-relations\n",
      "getting text from url https://www.lonza.com/investor-relations/agenda-and-events\n"
     ]
    }
   ],
   "source": [
    "text = get_text_from_website(\"https://www.lonza.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC Example: Perform new keywords discovery from www.lonza.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cell, patient-scale cell, patient-scale cell, patient-scale cell, calcium, nitrogen, ammonia, calcium, nitric acid, naphtha, niacin, vitamin, joint, niacinamide, mammalian cell cultures, mammalian cell culture, ucb, arabinogalactan, high-growth, u s, cerium, hydro-québec, isophthalic acid, amaxa  , amaxa, line, cell, joint, cell, l-carnitine, vitamin, gmp, glaxosmithkline, gsk, slough, cgmp, vitamin b3  , vitamin b3, human, mesoblast, mesoblast, stem cell, arch, gmp, gmp, urethanes, sartorius stedim, cell, sartorius stedim, niacinamide, wood, slough, hepatocyte, platinum, joint, retinal, l-asparaginase, patient, capsule, 3d, cannabinoids  , cancer  , cancer, cells, patients, antibody-drug, tumors, tumor cells, cancer cells, cells, patients, bioconjugates, bioconjugates, antibody-drug conjugates, vaccine conjugates, cells, cellular, cancerous cells, tumor, cell, tumor cells, cancer, patient, swiss, swiss, eth, human, chro, cell, cell, bioconjugates, cell, patients, cgmp, cell, cell, cocoon, cell, patients, 4d-nucleofector cell, non-viral cell, cell, network  , “, people, water, water, capsule, titanium dioxide-free, titanium dioxide-free, capsule, bispecific antibody, cis-targeted il-2, cis-targeted il-2, cell, cocoon, cell, cell, inhaled, nasal, seufert, seufert, ovarian cancer, cgmp, network, cd34+, mouse, human cord blood cd34+ hematopoietic stem cells, cd34+ cell, myeloma, myeloma, cer, cer1, cocoon, cocoon, patients, swiss, dna, head, kerstin meinecke, swiss, cell, l-carnitine, swiss, cell, faq, water, c, water, people, by-products, iso, ema, gmp, gmp, people, layer, patients, people, chf 1 7, full-year, patients, human, cell, philadelphia  , 27th, cell, cell, cell, heart, lungs, ddl, pulmonary, nasal, cell, people, line, cell, bioconjugates, cell, patients, cgmp, cell, cell, cocoon, cell, patients)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = get_text_from_website(\"https://www.lonza.com\")\n",
    "\n",
    "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
