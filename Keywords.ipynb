{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known Keyword Extracton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.request import Request, urlopen\n",
    "from tldextract import extract\n",
    "import progressbar\n",
    "import pandas as pd\n",
    "import certifi\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "ua = UserAgent()\n",
    "header = {'User-Agent':str(ua.chrome)}\n",
    "\n",
    "links_substring_blacklist = [\n",
    "    'sitemap',\n",
    "    'terms',\n",
    "    'contact',\n",
    "    'careers',\n",
    "    'uploads',\n",
    "    'policy',\n",
    "    '/media',\n",
    "    'events/'\n",
    "]\n",
    "keywords_limit = 15\n",
    "crawl_limit = 40\n",
    "\n",
    "def link_from_same_subdomain(url, domain):\n",
    "    _, sub_hostname, sub_suffix = extract(url)\n",
    "    _, hostname, suffix = extract(domain)\n",
    "    \n",
    "    return sub_hostname == hostname and sub_suffix == suffix\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    try:\n",
    "        html_page = requests.get(url, headers=header, verify=False).content\n",
    "    except Exception as e:\n",
    "        print('Failed to fetch', url, e, '\\n')\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "    for elem in soup.find_all(['script', 'li', 'header', 'noscript', 'h1', 'h2', 'h3', 'h4']):\n",
    "        elem.decompose()\n",
    "\n",
    "    strips = list(soup.stripped_strings)\n",
    "    \n",
    "    return '\\n'.join([f' {strip} ' for strip in strips if len(strip) > 50])\n",
    "\n",
    "def get_links_from_url(url):\n",
    "    try:\n",
    "        html_page = requests.get(url, headers=header, verify=False).content\n",
    "    except Exception as e:\n",
    "        print('faile to fetch', url, e, '\\n')\n",
    "        return set()\n",
    "\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "    # get all links with absolute paths\n",
    "    links = set()\n",
    "    for link in soup.findAll('a'):\n",
    "        link_href = urljoin(url, link.get('href'))\n",
    "        parsed = urlparse(link_href, scheme='https')._replace(fragment=\"\")\n",
    "        \n",
    "        if parsed.scheme[:4] == 'http':\n",
    "            links.add(parsed.geturl().rstrip('/'))\n",
    "\n",
    "    # return urls with same base domain\n",
    "    return {link for link in links if link_from_same_subdomain(link, url)}\n",
    "\n",
    "def get_links_recursive(url, existing_links=set(), max_depth=1, maxlen=20):\n",
    "    if (len(existing_links) >= max_depth):\n",
    "        return set()\n",
    "\n",
    "    links = get_links_from_url(url)\n",
    "    \n",
    "    existing_links = existing_links | {url}\n",
    "    for link in links - existing_links:\n",
    "        links |= get_links_recursive(link, existing_links)\n",
    "        \n",
    "    for blacklisted_domain_substring in links_substring_blacklist:\n",
    "        links = [link for link in links if blacklisted_domain_substring not in link]\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_text_from_website(website):\n",
    "    text = ''\n",
    "    for link in get_links_recursive(website)[:crawl_limit]:\n",
    "        print('getting', link, end='\\r')\n",
    "        text += get_text_from_url(link) + '\\n'\n",
    "        \n",
    "    return text.replace('.', ' ').lower()\n",
    "\n",
    "def get_known_keywords_for_website(website):\n",
    "    text = get_text_from_website(website)\n",
    "    known_keywords = pd.read_csv('known_keywords.csv').drop_duplicates()\n",
    "    known_keywords['keyword'] = known_keywords['keyword'].str.lower()\n",
    "    known_keywords['counts'] = known_keywords['keyword'].map(lambda x: text.count(f' {x.strip()} '))\n",
    "    known_keywords['keyword_word_count'] = known_keywords['keyword'].map(lambda x: len(x.split(' ')))\n",
    "\n",
    "    keyword_counts = known_keywords[known_keywords['counts'] > 0]\n",
    "    keyword_counts = keyword_counts.sort_values(by=['keyword_word_count', 'counts'], ascending=False)\n",
    "\n",
    "    return keyword_counts['keyword'].tolist()[:keywords_limit]\n",
    "\n",
    "def get_title(url):\n",
    "    _, hostname, suffix = extract(url)\n",
    "    return hostname.capitalize()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on a list of hanb-picked domains\n",
    "\n",
    "urls = [\n",
    "    'https://bionicalemas.com/',\n",
    "    'https://abzena.com/',\n",
    "    'https://lonza.com',\n",
    "    'https://www.merck.com/',\n",
    "    'https://www.eurofins.com/',\n",
    "    'https://www.sgs.com/'\n",
    "]\n",
    "\n",
    "extracted_keywords = []\n",
    "for url in urls:\n",
    "    extracted_keywords.append({\n",
    "        'domain': url,\n",
    "        'keywords': get_known_keywords_for_website(url),\n",
    "        'company_name': get_title(url)\n",
    "    })\n",
    "    \n",
    "extracted_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_kw = pd.DataFrame(extracted_keywords)\n",
    "extracted_kw.to_csv(\"extracted_keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import MultiTagger\n",
    "from flair.tokenization import SciSpacyTokenizer\n",
    "\n",
    "sentence = Sentence(\"Behavioral abnormalities in the Fmr1 KO2 Mouse Model of Fragile X Syndrome\")\n",
    "\n",
    "# load biomedical tagger\n",
    "tagger = MultiTagger.load(\"hunflair\")\n",
    "\n",
    "tags = []\n",
    "for s in text:\n",
    "    if s[:4] == 'http':\n",
    "        continue\n",
    "    sentence = Sentence(s)\n",
    "    tagger.predict(sentence)\n",
    "    print(s)\n",
    "    for annotation_layer in sentence.annotation_layers.keys():\n",
    "        for entity in sentence.get_spans(annotation_layer):\n",
    "            tags += [entity.text]\n",
    "            print('TAG!', entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
